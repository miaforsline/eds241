---
title: "Random Controlled Experiment: simulation of potential outcomes and randomly assigned treatement"
author: "Mia Forsline"
date: "1/27/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Set Up 

```{r}
library(MASS)
library(ggplot2)
library(vtable) #summar statistics 
library(stargazer)
library(estimatr)

set.seed(7307)
```


```{r}
bigN=20000 #sample size 

W <- runif(bigN,0,5) #the probabily of drawing any value from 0 - 5 is equal
X=as.integer(W+1)
X1 <- as.numeric(X==1) #define variables if X = 1, 2, 3, 4, or 5 
X2 <- as.numeric(X==2) 
X3 <- as.numeric(X==3)
X4 <- as.numeric(X==4)
X5 <- as.numeric(X==5)
```


```{r}
# GENERATE MEAN COMPONENT OF POTENTIAL OUTCOMES
MU0=(1/2)*X1 + (2/2)*X2 + (3/2)*X3 + (4/2)*X4 + (5/2)*X5 #define first step function 
mean(MU0)

MU1=1*X1 + 2*X2 + 3*X3 + 4*X4 + 5*X5 #define second step function 
mean(MU1)
```


```{r}
# GENERATE ERROR COMPONENT OF POTENTIAL OUTCOMES
Sigma <- matrix(c(1,0.75,0.75,1),2,2) #create covariance matrix w/mean = 0 
Sigma
e <- (mvrnorm(n=bigN, rep(0, 2), Sigma)) #draw 2 vector objects e0 and e1 (error part of potential outcomes)
e0 <- e[,c(1)]
mean(e0)
e1 <- e[,c(2)]  
mean(e1)
```


```{r}
# GENERATE POTENTIAL OUTCOMES Y(0) and Y(1)
Y0 <- MU0 + e0
mean(Y0)

Y1 <- MU1 + e1
mean(Y1)

ATE <- mean(Y1)-mean(Y0)
print(ATE)

#true ATE = 1.5 
#our calculated ATE = close to 1.5 

PO_DF <- data.frame(Y0,Y1,X) #potential outcome data frame 
```


```{r}
# PLOT POTENTIAL OUTCOMES AGAINST X, JITTER TO CLARIFY VISUAL
jitter <- position_jitter(width = 0.05, height = 0)
ggplot(PO_DF, aes(x=X, y=Y0)) +
  geom_point(color = "blue") +
  geom_point(position = jitter, color = "red", shape=1, aes(x=X, y=Y1))
# x only takes 5 discrete values 
#Y0 (blue) is lower than Y1 (red)
#shows the positive step functions we built 
#shows the hypothetical Y0 and Y1 values we created artificially
```

# Next, we will randomly assign a treatment 
```{r}
# RANDOMLY ASSIGN A TREATMENT INDICATOR
D <- as.numeric((runif(bigN,0,1)) > 0.5) #draw random number between 0 -1
#that number will be 0 if it's < 0.5 (control group)
#that number will be 1 if it's > 0.5 (treatment group)
mean(D) #the avg is 0.5, as we expect --> balanced control and treatment groups
```

After we run our experiment, we collect our observed data (in our case, Y is generated through our model)
```{r}
# USE SUTVA TO MAP POTENTIAL OUTCOMES INTO OBSERVED OUTCOMES
Y = D*Y1 + (1-D)*Y0
#if D = 0, we will observe Y(0)
#if D = 1, we will observe Y(1)

# COLLECT ALL RELEVANT VARIABLES IN A DATAFRAME
RCT_DATA <- data.frame(Y, D, Y0, Y1, X, X1, X2, X3, X4, X5)
#we've included potential outcomes (but in the real world we would not be able to observe them all)
```

# Check that D (treatment) is independent of X (and Y(0), and Y(1))
- baseline pre-treatment characteristics must be balanced 
```{r}
# CHECK THAT D IS INDEPENDENT OF X, Y0, Y1 (RECALL Y0,Y1 NOT OBSERVED IN REALITY)
# "TEST" OF COVARIATE BALANCE
sumtable(RCT_DATA, vars=c('Y0','Y1', 'Y', 'X1', 'X2', 'X3', 'X4', 'X5'), group='D', group.test=TRUE)
#F-test - little stars mean we can reject our null hypothesis 
#N = we expect roughly half of our sample in the control and treatment groups, which is what we see at 9953 and 10047 
#if D is not independent, the groups will not be balanced
#means should also be fairly similar
#Y means differ by 1.5 
```

Linear regressions - regress X (and Y(0) and Y(1)) on D 
- another way to check for randomization 
```{r}
mA <- lm(formula = X ~ D, data=RCT_DATA) #X is from 1 - 5 
mB <- lm(formula = Y0 ~ D, data=RCT_DATA)
mC <- lm(formula = Y1 ~ D, data=RCT_DATA)
se_models = starprep(mA, mB, mC, stat = c("std.error"), se_type = "HC2", alpha = 0.05)
stargazer(mA, mB, mC, se = se_models, type="text")
```


```{r}
# ESTIMATE ATE USING SIMPLE OLS REGRESSION OF Y on D
ate1 <- lm(formula = Y ~ D, data=RCT_DATA)
ate2 <- lm(formula = Y ~ D + X, data=RCT_DATA)
se_models = starprep(ate1, ate2, stat = c("std.error"), se_type = "HC2", alpha = 0.05)
stargazer(ate1, ate2, se = se_models, type="text")

#SE decreases when we include X 
##sum of squared fitted residuals of the regression = smaller in the model with more regressors --> this is how we calculate SE 
#R2 also increases when we include more regressors in the model 
#when you add regressors, the explanatory power of your model must go up or stay the same (R2 mechanically goes up --> sometimes we need an adjusted R2 value to prevent us from overfitting the data)


```

